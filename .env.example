# ============================================================================
# CRYPTO-PULLER CONFIGURATION
# ============================================================================
# Copy this file to .env and configure with your values
# cp .env.example .env

# ============================================================================
# DATABASE
# ============================================================================
DATABASE_URL=postgresql://user:password@localhost/crypto_puller

# ============================================================================
# SERVER PORTS
# ============================================================================
HTTP_PORT=3000
GRPC_PORT=50051

# ============================================================================
# LOGGING
# ============================================================================
# Options: trace, debug, info, warn, error
LOG_LEVEL=info

# ============================================================================
# EVENT OUTPUT (Sink Configuration)
# ============================================================================
# If true: outputs events to console (good for development/testing)
# If false: sends events to Kafka (production)
USE_CONSOLE_INSTEAD_KAFKA=true

# Kafka Configuration (required if USE_CONSOLE_INSTEAD_KAFKA=false)
KAFKA_BROKERS=localhost:9092
KAFKA_TOPIC=usdt_transfers

# ============================================================================
# BLOCKCHAIN SCANNERS - ENABLE/DISABLE
# ============================================================================
ENABLE_TRON=true
ENABLE_TON=true
ENABLE_ETHEREUM=true

# ============================================================================
# OPTIMIZATION SETTINGS
# ============================================================================
# Batch Sizes: Number of blocks to process per RPC request
# Higher = faster but more memory, may hit RPC limits
# Lower = slower but more reliable, less memory

# Ethereum: 100 blocks recommended (Infura max: 10000 logs)
ETHEREUM_BATCH_SIZE=100

# TRON: 50 blocks recommended
TRON_BATCH_SIZE=5

# TON: 10 blocks recommended (TON API is slower)
TON_BATCH_SIZE=10

# Parallel Processing: Number of concurrent batch requests
# Higher = faster but may exceed rate limits
# Lower = slower but more reliable
MAX_CONCURRENT_REQUESTS=5

# Timeout for each batch request (seconds)
BATCH_TIMEOUT_SECS=30

# ============================================================================
# ETHEREUM CONFIGURATION
# ============================================================================
# RPC URL - Choose your provider:
# - GetBlock: https://go.getblock.io/<YOUR_API_KEY>
# - Infura: https://mainnet.infura.io/v3/<YOUR_API_KEY>
# - Alchemy: https://eth-mainnet.g.alchemy.com/v2/<YOUR_API_KEY>
# - QuickNode: https://<YOUR_ENDPOINT>.quiknode.pro/<YOUR_API_KEY>
ETHEREUM_RPC_URL=https://go.getblock.io/YOUR_API_KEY

# Start scanning from specific timestamp (optional)
# Format: ISO 8601 (YYYY-MM-DDTHH:MM:SSZ)
# If not set, starts from latest block
START_FROM_ETHEREUM=2025-01-01T00:00:00Z

# ============================================================================
# TRON CONFIGURATION
# ============================================================================
# RPC URL
# - TronGrid: https://api.trongrid.io
# - TronStack: https://api.tronstack.io
# - Custom EVM-compatible endpoint
TRON_RPC_URL=https://api.trongrid.io

# API Key (optional, for TronGrid)
# TRON_RPC_KEY=your_trongrid_api_key

# Start scanning from specific timestamp (optional)
START_FROM_TRON=2025-01-01T00:00:00Z

# ============================================================================
# TON CONFIGURATION
# ============================================================================
# RPC URL
# - QuickNode: https://<your-subdomain>.ton-mainnet.quiknode.pro/<token>
# - Toncenter: https://toncenter.com/api/v2/jsonRPC
TON_RPC_URL=https://your-endpoint.ton-mainnet.quiknode.pro/YOUR_TOKEN

# API Key (required for Toncenter, not needed for QuickNode)
TON_API_KEY=your_toncenter_api_key

# Start scanning from specific timestamp (optional)
START_FROM_TON=2025-01-01T00:00:00Z

# ============================================================================
# WALLET MANAGEMENT
# ============================================================================
# NOTE: Wallets are NOT configured via environment variables!
# They are managed dynamically through:
#
# 1. Database (wallets table):
#    INSERT INTO wallets (chain, address) VALUES ('"Ethereum"', '0x...');
#
# 2. HTTP API:
#    curl -X POST http://localhost:3000/wallets \
#      -H "Content-Type: application/json" \
#      -d '{"chain": "Ethereum", "address": "0x..."}'
#
# 3. gRPC Service:
#    grpcurl -plaintext \
#      -d '{"chain": "Ethereum", "address": "0x..."}' \
#      localhost:50051 wallet.WalletService/AddWallet
#
# 4. Kafka Consumer (if USE_CONSOLE_INSTEAD_KAFKA=false):
#    Send JSON: {"chain": "Ethereum", "address": "0x..."}
#    to the configured KAFKA_TOPIC
#
# This design allows adding/removing wallets without restart!

# ============================================================================
# USAGE EXAMPLES
# ============================================================================

# Example 1: Real-time monitoring (low latency, small batches)
# ETHEREUM_BATCH_SIZE=20
# MAX_CONCURRENT_REQUESTS=2
# USE_CONSOLE_INSTEAD_KAFKA=false

# Example 2: Historical rescan (high throughput, large batches)
# ETHEREUM_BATCH_SIZE=100
# MAX_CONCURRENT_REQUESTS=10
# USE_CONSOLE_INSTEAD_KAFKA=true

# Example 3: Conservative (maximum reliability)
# ETHEREUM_BATCH_SIZE=10
# MAX_CONCURRENT_REQUESTS=1
# BATCH_TIMEOUT_SECS=60

# ============================================================================
# NOTES
# ============================================================================
# - Wallets are loaded from database on startup via load_wallets()
# - Wallets can be added dynamically via API without restart
# - Batch sizes affect memory usage: larger batches = more memory
# - Rate limits vary by provider - adjust MAX_CONCURRENT_REQUESTS accordingly
# - For production, use Kafka output (USE_CONSOLE_INSTEAD_KAFKA=false)
# - Monitor metrics at http://localhost:9090/metrics
